{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T21:10:40.209775Z",
     "start_time": "2020-10-09T21:10:16.313229Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from os.path import join, dirname, abspath\n",
    "\n",
    "import numpy as np\n",
    "import pandas  as pd\n",
    "import spacy\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T21:10:40.249804Z",
     "start_time": "2020-10-09T21:10:40.213586Z"
    }
   },
   "outputs": [],
   "source": [
    "def json_to_pd(r_path):\n",
    "    dic = json.load(open(r_path, 'r'))\n",
    "    cols = list(dic['train'][0])+['split']\n",
    "    \n",
    "    data_train = np.array([np.array(list(i.values())+['train']) for i in dic['train']])\n",
    "    data_test = np.array([np.array(list(i.values())+['test']) for i in dic['test']])\n",
    "    \n",
    "    ind_train = [i['id'] for i in dic['train']]\n",
    "    ind_test = [i['id'] for i in dic['test']]\n",
    "    \n",
    "    json_df_train = pd.DataFrame(data=data_train, index=ind_train, columns=cols)\n",
    "    json_df_train['risco'] = pd.to_numeric(json_df_train['risco'])\n",
    "    json_df_train['date'] = pd.to_datetime(json_df_train['date'])\n",
    "    json_df_train = json_df_train.filter(['txt', 'risco', 'split'])\n",
    "    \n",
    "    json_df_test = pd.DataFrame(data=data_test, index=ind_test, columns=cols)\n",
    "    json_df_test['risco'] = pd.to_numeric(json_df_test['risco'])\n",
    "    json_df_test['date'] = pd.to_datetime(json_df_test['date'])\n",
    "    json_df_test = json_df_test.filter(['txt', 'risco', 'split'])\n",
    "    return json_df_train, json_df_test\n",
    "# Entrada: Caminho até o arquivo e nome do arquivo\n",
    "# Saída: hdf/dataframe com vetores de frequência, risco e split\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T21:10:40.492217Z",
     "start_time": "2020-10-09T21:10:40.254549Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_to_tfidf_vectors(filename, path_to_folder= '/media/training/crossvalR/'):\n",
    "    \"\"\" Receives a filename for json file and a path to folder.\n",
    "        Returns a dataframe with text vectors instead of\n",
    "        If no path_to_folder is given, the default folder is inside\n",
    "        resources/data.\n",
    "    \"\"\"\n",
    "    if not path_to_folder:\n",
    "        path_to_folder = join(abspath(dirname(__file__)), 'resources/data/')\n",
    "\n",
    "    VECTOR_MODEL_NAME = \"pt_core_news_sm\"\n",
    "    NLP_SPACY = spacy.load(VECTOR_MODEL_NAME)\n",
    "    TARGET_VARIABLE = \"RISCO\"\n",
    "    TEXT_VARIABLE = \"TXT\"\n",
    "\n",
    "    path_to_file = path_to_folder + filename + \".json\"\n",
    "\n",
    "    data_df_train, data_df_test = json_to_pd(path_to_file)\n",
    "\n",
    "    ''' Create the pipeline 'sentencizer' component '''\n",
    "    sentencizer = NLP_SPACY.create_pipe('sentencizer')\n",
    "    try:\n",
    "        ''' We then add the component to the pipeline if we hadn't done before '''\n",
    "        NLP_SPACY.add_pipe(sentencizer, before='parser')\n",
    "    except ValueError:\n",
    "        print(\"Pipe already present.\")\n",
    "\n",
    "    for data_df in [data_df_train, data_df_test]:\n",
    "        # Renaming the columns\n",
    "        # Let's start uppercasing all column names and target variable values\n",
    "        data_df.columns = map(lambda x: str(x).upper(), data_df.columns)\n",
    "        data_df[TARGET_VARIABLE] = data_df[TARGET_VARIABLE].apply(\n",
    "            lambda x: str(x))\n",
    "\n",
    "        # print(data_df.head())\n",
    "\n",
    "        # Removing ponctuation and stopwords\n",
    "        # As we can see, we have a lot of tokens from text variable being\n",
    "        # ponctuations or words that don't have by themselves much meaning.\n",
    "        # We're going to load a built-in stopwords list to remove these\n",
    "        # unnecessary tokens.\n",
    "        stopwords_set = set(STOP_WORDS).union(\n",
    "            set(stopwords.words('portuguese'))).union(\n",
    "                set(['anos', 'ano', 'dia', 'dias', 'nº', 'n°']))\n",
    "\n",
    "        # Removing HTML\n",
    "        data_df['TXT'] = data_df['TXT'].str.replace(r'<.*?>', '')\n",
    "\n",
    "        # Lemmatizing and stemming\n",
    "        # print(\"This is the stopword list: \", sorted(list(stopwords_set)))\n",
    "\n",
    "        ''' Not all variables are being undestood as strings so we have to force it'''\n",
    "        preprocessed_text_data = data_df[TEXT_VARIABLE].to_list()\n",
    "\n",
    "\n",
    "        # print(NLP_SPACY.pipe_names)\n",
    "\n",
    "        tokenized_data = []\n",
    "        semantics_data = []\n",
    "        lemmatized_doc = []\n",
    "        normalized_doc = []\n",
    "        raw_doc = []\n",
    "        for row in preprocessed_text_data:\n",
    "            doc = NLP_SPACY(row)\n",
    "            preprocessed_doc = [\n",
    "                token for token in doc\n",
    "                if token.is_alpha and token.norm_ not in stopwords_set]\n",
    "            tokenized_data.append(preprocessed_doc)\n",
    "            raw_doc.append(\" \".join([word.text for word in preprocessed_doc]))\n",
    "            lemmatized_doc.append(\n",
    "                \" \".join([word.lemma_ for word in preprocessed_doc]))\n",
    "            normalized_doc.append(\n",
    "                \" \".join([word.norm_ for word in preprocessed_doc]))\n",
    "\n",
    "        data_df['RAW_DOC'] = raw_doc\n",
    "        data_df['NORMALIZED_DOC'] = normalized_doc\n",
    "        data_df['LEMMATIZED_DOC'] = lemmatized_doc\n",
    "\n",
    "    #     print(data_df.head())\n",
    "\n",
    "        # Entity recognition and filtering\n",
    "        # Some parts of speech may mislead the model associating classes\n",
    "        # to certain entities that are not really related to the categories.\n",
    "        processed_tokenized_data = []\n",
    "        processed_doc_text = []\n",
    "        entities_obs = []\n",
    "        entity_unwanted_types = set(['PER', 'ORG'])\n",
    "\n",
    "        for doc in tokenized_data:\n",
    "            entities_text = \"\"\n",
    "            processed_doc = []\n",
    "            for token in doc:\n",
    "                if not token.ent_type_:\n",
    "                    processed_doc.append(token)\n",
    "                elif token.ent_type_ not in entity_unwanted_types:\n",
    "                    processed_doc.append(token)\n",
    "                    entities_obs.append((token.text, token.ent_type_))\n",
    "\n",
    "            processed_tokenized_data.append(processed_doc)\n",
    "            processed_doc_text.append(\n",
    "                \" \".join([word.norm_ for word in processed_doc]))\n",
    "\n",
    "        ''' Processing text on entity level'''\n",
    "        data_df['PROCESSED_DOC'] = processed_doc_text\n",
    "        \n",
    "        # print(data_df.head())\n",
    "\n",
    "        # Now we're going to remove POS,\n",
    "        # only allowing proper nouns, nouns, adjectives, adverbs\n",
    "        # and verb to present in our text variable.\n",
    "\n",
    "        allowed_pos_set = set([\"PROPN\", \"NOUN\", \"ADV\", \"ADJ\", \"VERB\"])\n",
    "\n",
    "        processed_doc = []\n",
    "        filtered_token_obs = []\n",
    "        for doc in processed_tokenized_data:\n",
    "            doc_tokens = [word for word in doc if str(word.pos_) in allowed_pos_set]\n",
    "            filtered_token_obs.append(doc_tokens)\n",
    "            processed_doc.append(\" \".join(token.norm_ for token in doc_tokens))\n",
    "\n",
    "        data_df['PROCESSED_DOC'] = processed_doc\n",
    "        data_df['TOKENS'] = filtered_token_obs\n",
    "        # print(data_df.head()) \n",
    "\n",
    "        # Removing extra spaces originated from the removal of tokens\n",
    "        space_pattern = r'\\s\\s+'\n",
    "        data_df['PROCESSED_DOC'] = data_df['PROCESSED_DOC'].str.replace(space_pattern, \" \").str.strip()\n",
    "        data_df = data_df\n",
    "        data_df = data_df.drop(columns=['TOKENS']).dropna()\n",
    "        data_df[TARGET_VARIABLE] = data_df[TARGET_VARIABLE].apply(lambda x: str(x))\n",
    "\n",
    "        # Removing accents and symbols\n",
    "        data_df['PROCESSED_DOC'] = data_df['PROCESSED_DOC'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "\n",
    "        ''' Best parameter using GridSearch (CV score=0.535): \n",
    "        {'tfidf__norm': 'l2', 'tfidf__smooth_idf': False, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True,\n",
    "        'vect__max_df': 0.2, 'vect__max_features': None, 'vect__min_df': 0.0006, 'vect__ngram_range': (1, 3)}\n",
    "        Those were obtained on the next code block.\n",
    "        '''\n",
    "        count_vectorizer = CountVectorizer(\n",
    "            max_features=None, min_df=0.0006, max_df=0.2, ngram_range=(1, 3))\n",
    "        tfidf_transformer = TfidfTransformer(\n",
    "            norm='l2', use_idf=True, sublinear_tf=False)\n",
    "\n",
    "        # First let's split train and test data\n",
    "        train_mask = data_df['SPLIT'] == 'train'\n",
    "        test_mask = data_df['SPLIT'] == 'test'\n",
    "\n",
    "        train_df = data_df[train_mask]\n",
    "        test_df = data_df[test_mask]\n",
    "        ''' Let's transform the lemmatized documents into count vectors '''\n",
    "        train_count_vectors = count_vectorizer.fit_transform(\n",
    "            train_df['PROCESSED_DOC'])\n",
    "        test_count_vectors = count_vectorizer.transform(\n",
    "            test_df['PROCESSED_DOC'])\n",
    "\n",
    "        ''' Then use those count vectors to generate frequency vectors '''\n",
    "        train_frequency_vectors = tfidf_transformer.fit_transform(\n",
    "            train_count_vectors)\n",
    "        test_frequency_vectors = tfidf_transformer.transform(\n",
    "            test_count_vectors)\n",
    "\n",
    "        return train_frequency_vectors, test_frequency_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T21:10:40.725319Z",
     "start_time": "2020-10-09T21:10:40.494693Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_sparse_csr(filename, array):\n",
    "    np.savez(filename, data=array.data, indices=array.indices,\n",
    "             indptr=array.indptr, shape=array.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T21:10:44.826468Z",
     "start_time": "2020-10-09T21:10:40.733226Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/media/training/crossvalR/dic_raw_0_0.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a3107d0eedc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_to_tfidf_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dic_raw_0_0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-d637cf06872c>\u001b[0m in \u001b[0;36mtext_to_tfidf_vectors\u001b[0;34m(filename, path_to_folder)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mpath_to_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_folder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mdata_df_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_df_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_to_pd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;34m''' Create the pipeline 'sentencizer' component '''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-3dc6d710f4b0>\u001b[0m in \u001b[0;36mjson_to_pd\u001b[0;34m(r_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mjson_to_pd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'split'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdata_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/media/training/crossvalR/dic_raw_0_0.json'"
     ]
    }
   ],
   "source": [
    "tfidf = text_to_tfidf_vectors('dic_raw_0_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T21:10:44.828098Z",
     "start_time": "2020-10-09T21:10:22.891Z"
    }
   },
   "outputs": [],
   "source": [
    "path_to_file = \"\"\n",
    "save_sparse_csr(\"tfidf_vectors_0_0_train\", tfidf[0])\n",
    "save_sparse_csr(\"tfidf_vectors_0_0_test\", tfidf[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
