{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os.path import join, dirname, abspath\n",
    "\n",
    "import numpy as np\n",
    "import pandas  as pd\n",
    "import spacy\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_pd(r_path):\n",
    "    dic = json.load(open(r_path, 'r'))\n",
    "    cols = list(dic['train'][0])+['split']\n",
    "    \n",
    "    data_train = np.array([np.array(list(i.values())+['train']) for i in dic['train']])\n",
    "    data_test = np.array([np.array(list(i.values())+['test']) for i in dic['test']])\n",
    "    \n",
    "    ind_train = [i['id'] for i in dic['train']]\n",
    "    ind_test = [i['id'] for i in dic['test']]\n",
    "    \n",
    "    json_df_train = pd.DataFrame(data=data_train, index=ind_train, columns=cols)\n",
    "    json_df_train['risco'] = pd.to_numeric(json_df_train['risco'])\n",
    "    json_df_train['date'] = pd.to_datetime(json_df_train['date'])\n",
    "    json_df_train = json_df_train.filter(['txt', 'risco', 'split'])\n",
    "    \n",
    "    json_df_test = pd.DataFrame(data=data_test, index=ind_test, columns=cols)\n",
    "    json_df_test['risco'] = pd.to_numeric(json_df_test['risco'])\n",
    "    json_df_test['date'] = pd.to_datetime(json_df_test['date'])\n",
    "    json_df_test = json_df_test.filter(['txt', 'risco', 'split'])\n",
    "    return json_df_train, json_df_test\n",
    "# Entrada: Caminho até o arquivo e nome do arquivo\n",
    "# Saída: hdf/dataframe com vetores de frequência, risco e split\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_tfidf_vectors(filename, path_to_folder= '/media/training/crossvalR/'):\n",
    "    \"\"\" Receives a filename for json file and a path to folder.\n",
    "        Returns a dataframe with text vectors instead of\n",
    "        If no path_to_folder is given, the default folder is inside\n",
    "        resources/data.\n",
    "    \"\"\"\n",
    "    if not path_to_folder:\n",
    "        path_to_folder = join(abspath(dirname(__file__)), 'resources/data/')\n",
    "\n",
    "    VECTOR_MODEL_NAME = \"pt_core_news_sm\"\n",
    "    NLP_SPACY = spacy.load(VECTOR_MODEL_NAME)\n",
    "    TARGET_VARIABLE = \"RISCO\"\n",
    "    TEXT_VARIABLE = \"TXT\"\n",
    "\n",
    "    path_to_file = path_to_folder + filename + \".json\"\n",
    "\n",
    "    data_df_train, data_df_test = json_to_pd(path_to_file)\n",
    "\n",
    "    ''' Create the pipeline 'sentencizer' component '''\n",
    "    sentencizer = NLP_SPACY.create_pipe('sentencizer')\n",
    "    try:\n",
    "        ''' We then add the component to the pipeline if we hadn't done before '''\n",
    "        NLP_SPACY.add_pipe(sentencizer, before='parser')\n",
    "    except ValueError:\n",
    "        print(\"Pipe already present.\")\n",
    "\n",
    "    for data_df in [data_df_train, data_df_test]:\n",
    "        # Renaming the columns\n",
    "        # Let's start uppercasing all column names and target variable values\n",
    "        data_df.columns = map(lambda x: str(x).upper(), data_df.columns)\n",
    "        data_df[TARGET_VARIABLE] = data_df[TARGET_VARIABLE].apply(\n",
    "            lambda x: str(x))\n",
    "\n",
    "        # print(data_df.head())\n",
    "\n",
    "        # Removing ponctuation and stopwords\n",
    "        # As we can see, we have a lot of tokens from text variable being\n",
    "        # ponctuations or words that don't have by themselves much meaning.\n",
    "        # We're going to load a built-in stopwords list to remove these\n",
    "        # unnecessary tokens.\n",
    "        stopwords_set = set(STOP_WORDS).union(\n",
    "            set(stopwords.words('portuguese'))).union(\n",
    "                set(['anos', 'ano', 'dia', 'dias', 'nº', 'n°']))\n",
    "\n",
    "        # Removing HTML\n",
    "        data_df['TXT'] = data_df['TXT'].str.replace(r'<.*?>', '')\n",
    "\n",
    "        # Lemmatizing and stemming\n",
    "        # print(\"This is the stopword list: \", sorted(list(stopwords_set)))\n",
    "\n",
    "        ''' Not all variables are being undestood as strings so we have to force it'''\n",
    "        preprocessed_text_data = data_df[TEXT_VARIABLE].to_list()\n",
    "\n",
    "\n",
    "        # print(NLP_SPACY.pipe_names)\n",
    "\n",
    "        tokenized_data = []\n",
    "        semantics_data = []\n",
    "        lemmatized_doc = []\n",
    "        normalized_doc = []\n",
    "        raw_doc = []\n",
    "        for row in preprocessed_text_data:\n",
    "            doc = NLP_SPACY(row)\n",
    "            preprocessed_doc = [\n",
    "                token for token in doc\n",
    "                if token.is_alpha and token.norm_ not in stopwords_set]\n",
    "            tokenized_data.append(preprocessed_doc)\n",
    "            raw_doc.append(\" \".join([word.text for word in preprocessed_doc]))\n",
    "            lemmatized_doc.append(\n",
    "                \" \".join([word.lemma_ for word in preprocessed_doc]))\n",
    "            normalized_doc.append(\n",
    "                \" \".join([word.norm_ for word in preprocessed_doc]))\n",
    "\n",
    "        data_df['RAW_DOC'] = raw_doc\n",
    "        data_df['NORMALIZED_DOC'] = normalized_doc\n",
    "        data_df['LEMMATIZED_DOC'] = lemmatized_doc\n",
    "\n",
    "    #     print(data_df.head())\n",
    "\n",
    "        # Entity recognition and filtering\n",
    "        # Some parts of speech may mislead the model associating classes\n",
    "        # to certain entities that are not really related to the categories.\n",
    "        processed_tokenized_data = []\n",
    "        processed_doc_text = []\n",
    "        entities_obs = []\n",
    "        entity_unwanted_types = set(['PER', 'ORG'])\n",
    "\n",
    "        for doc in tokenized_data:\n",
    "            entities_text = \"\"\n",
    "            processed_doc = []\n",
    "            for token in doc:\n",
    "                if not token.ent_type_:\n",
    "                    processed_doc.append(token)\n",
    "                elif token.ent_type_ not in entity_unwanted_types:\n",
    "                    processed_doc.append(token)\n",
    "                    entities_obs.append((token.text, token.ent_type_))\n",
    "\n",
    "            processed_tokenized_data.append(processed_doc)\n",
    "            processed_doc_text.append(\n",
    "                \" \".join([word.norm_ for word in processed_doc]))\n",
    "\n",
    "        ''' Processing text on entity level'''\n",
    "        data_df['PROCESSED_DOC'] = processed_doc_text\n",
    "        \n",
    "        # print(data_df.head())\n",
    "\n",
    "        # Now we're going to remove POS,\n",
    "        # only allowing proper nouns, nouns, adjectives, adverbs\n",
    "        # and verb to present in our text variable.\n",
    "\n",
    "        allowed_pos_set = set([\"PROPN\", \"NOUN\", \"ADV\", \"ADJ\", \"VERB\"])\n",
    "\n",
    "        processed_doc = []\n",
    "        filtered_token_obs = []\n",
    "        for doc in processed_tokenized_data:\n",
    "            doc_tokens = [word for word in doc if str(word.pos_) in allowed_pos_set]\n",
    "            filtered_token_obs.append(doc_tokens)\n",
    "            processed_doc.append(\" \".join(token.norm_ for token in doc_tokens))\n",
    "\n",
    "        data_df['PROCESSED_DOC'] = processed_doc\n",
    "        data_df['TOKENS'] = filtered_token_obs\n",
    "        # print(data_df.head()) \n",
    "\n",
    "        # Removing extra spaces originated from the removal of tokens\n",
    "        space_pattern = r'\\s\\s+'\n",
    "        data_df['PROCESSED_DOC'] = data_df['PROCESSED_DOC'].str.replace(space_pattern, \" \").str.strip()\n",
    "        data_df = data_df\n",
    "        data_df = data_df.drop(columns=['TOKENS']).dropna()\n",
    "        data_df[TARGET_VARIABLE] = data_df[TARGET_VARIABLE].apply(lambda x: str(x))\n",
    "\n",
    "        # Removing accents and symbols\n",
    "        data_df['PROCESSED_DOC'] = data_df['PROCESSED_DOC'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "\n",
    "        # print(data_df.info())\n",
    "        ''' Best parameter using GridSearch (CV score=0.535): \n",
    "        {'tfidf__norm': 'l2', 'tfidf__smooth_idf': False, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True,\n",
    "        'vect__max_df': 0.2, 'vect__max_features': None, 'vect__min_df': 0.0006, 'vect__ngram_range': (1, 3)}\n",
    "        Those were obtained on the next code block.\n",
    "        '''\n",
    "\n",
    "    tfidf_transformer = TfidfVectorizer(\n",
    "        norm='l2', use_idf=True, sublinear_tf=False)\n",
    "\n",
    "    ''' Then use those count vectors to generate frequency vectors '''\n",
    "    frequency_vectors_train = tfidf_transformer.fit_transform(data_df_train['PROCESSED_DOC'])\n",
    "    frequency_vectors_test = tfidf_transformer.transform(data_df_test['PROCESSED_DOC'])\n",
    "    \n",
    "    return frequency_vectors_train, frequency_vectors_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sparse_csr(filename, array):\n",
    "    np.savez(filename, data=array.data, indices=array.indices,\n",
    "             indptr=array.indptr, shape=array.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 33s, sys: 1.3 s, total: 6min 35s\n",
      "Wall time: 6min 35s\n"
     ]
    }
   ],
   "source": [
    "tfidf = text_to_tfidf_vectors('dic_raw_0_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = \"\"\n",
    "save_sparse_csr(\"tfidf_vectors_0_0_train\", tfidf[0])\n",
    "save_sparse_csr(\"tfidf_vectors_0_0_test\", tfidf[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
